{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnD1Ci7eiD0D"
   },
   "source": [
    "# Task 1: Fine-tune Chemical Language Model\n",
    "\n",
    "The goal is to fine-tune a pre-trained chemical language model on a regression task using the Lipophilicity dataset. The task involves predicting the lipophilicity value for a given molecule representation (SMILES string). You will learn how to load and tokenize a dataset from HuggingFace, how to load a pre-trained language model, and finally, how to run a model in inference mode.\n",
    "\n",
    "Your task is to complete the missing code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jD4OFgoEF2r4",
    "outputId": "2a4402b1-5b8e-43aa-a165-dd5a1c5554ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.3.2)\n",
      "Collecting accelerate==0.26.0 (from -r requirements.txt (line 2))\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (3.9.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.49.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (0.5.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 7)) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
      "Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.4.0\n",
      "    Uninstalling accelerate-1.4.0:\n",
      "      Successfully uninstalled accelerate-1.4.0\n",
      "Successfully installed accelerate-0.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R1DT0jiaiBE_"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "moTcuSAWabFV"
   },
   "outputs": [],
   "source": [
    "# setting the seed\n",
    "seed = 100\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dd7mQ25RZbe"
   },
   "source": [
    "# 1.Fine-tune a Chemical Language Model on Lipophilicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4_UNNRoie5t"
   },
   "source": [
    "\n",
    "## --- Step 1: Load Dataset ---\n",
    "\n",
    "The dataset we are going to use is the [Lipophilicity](https://huggingface.co/datasets/scikit-fingerprints/MoleculeNet_Lipophilicity) dataset, part of [MoleculeNet](https://pubs.rsc.org/en/content/articlelanding/2018/sc/c7sc02664a) benchmark.\n",
    "\n",
    "Lipophilicity, also known as hydrophobicity, is a measure of how readily a substance dissolves in nonpolar solvents (such as oil) compared to polar solvents (such as water)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oYg7_utTqmuB"
   },
   "outputs": [],
   "source": [
    "# specify dataset name and model name\n",
    "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
    "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"  #MoLFormer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GziJrmn-lirY",
    "outputId": "eeb27418-f9e8-4641-b364-54173eb6bdce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from HuggingFace\n",
    "import pandas as pd\n",
    "lipophilicity_df = pd.read_csv(\"hf://datasets/scikit-fingerprints/MoleculeNet_Lipophilicity/lipophilicity.csv\")\n",
    "\n",
    "#dataset = load_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmzmTPdamCe8",
    "outputId": "4520206b-9591-485f-f5d2-1483ebc7da59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4200 entries, 0 to 4199\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   SMILES  4200 non-null   object \n",
      " 1   label   4200 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 65.8+ KB\n",
      "None\n",
      "                                              SMILES  label\n",
      "0            Cn1c(CN2CCN(CC2)c3ccc(Cl)cc3)nc4ccccc14   3.54\n",
      "1  COc1cc(OC)c(cc1NC(=O)CSCC(=O)O)S(=O)(=O)N2C(C)...  -1.18\n",
      "2             COC(=O)[C@@H](N1CCc2sccc2C1)c3ccccc3Cl   3.69\n",
      "3  OC[C@H](O)CN1C(=O)C(Cc2ccccc12)NC(=O)c3cc4cc(C...   3.37\n",
      "4  Cc1cccc(C[C@H](NC(=O)c2cc(nn2C)C(C)(C)C)C(=O)N...   3.10\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "# For example, print the column names and display a few sample rows\n",
    "# TODO: your code goes here\n",
    "print(lipophilicity_df.shape)\n",
    "print(lipophilicity_df.info())\n",
    "print(lipophilicity_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zNHpj5bIabFW"
   },
   "outputs": [],
   "source": [
    "lipophilicity_strings = lipophilicity_df['SMILES'].values\n",
    "lipophilicity_targets = lipophilicity_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdjfUSH9iN2H",
    "outputId": "3e650770-2281-4eee-da57-98199479b5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS(=O)(=O)c1ccc(Oc2ccc(cc2)C#C[C@]3(O)CN4CCC3CC4)cc1', 1.51]\n"
     ]
    }
   ],
   "source": [
    "# define a PyTorch Dataset class for handling SMILES strings and targets\n",
    "\n",
    "# TODO: your code goes here\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, strings, labels):\n",
    "        self.strings = strings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        string = self.strings[idx]\n",
    "        target = self.labels[idx]\n",
    "        return [string, target]\n",
    "\n",
    "#printing a sample to check whether it is working or not\n",
    "smile_dataset = SMILESDataset(lipophilicity_strings, lipophilicity_targets)\n",
    "print(smile_dataset[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKptTi3jiikc"
   },
   "source": [
    "## --- Step 2: Split Dataset ---\n",
    "\n",
    "As there is only one split (train split) in the original dataset, we need to split the data into training and testing sets by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lpXODyAqp6zS"
   },
   "outputs": [],
   "source": [
    "# tokenize the data\n",
    "# load a pre-trained tokenizer from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K0y18iVBiTBa"
   },
   "outputs": [],
   "source": [
    "# split the data into training and test datasets\n",
    "# TODO: your code goes here\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#X_train and X_test contains strings and targets\n",
    "scaler = MinMaxScaler()\n",
    "# scaling the target values\n",
    "lipophilicity_targets = scaler.fit_transform(lipophilicity_targets.reshape(-1,1))\n",
    "X_train, X_test, y_train_scaled, y_test_scaled = train_test_split(lipophilicity_strings, lipophilicity_targets, test_size = 0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cMYnJmYHmquA"
   },
   "outputs": [],
   "source": [
    "# construct Pytorch data loaders for both train and test datasets\n",
    "BATCH_SIZE = 16 # adjust based on memory constraints\n",
    "\n",
    "# TODO: your code goes here\n",
    "# creating train dataloader\n",
    "train_dataset = SMILESDataset(X_train, y_train_scaled)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# creating test dataloader\n",
    "test_dataset = SMILESDataset(X_test, y_test_scaled)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn4xaN3KikNy"
   },
   "source": [
    "## --- Step 3: Load Model ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wT9Qn6PIrMwi",
    "outputId": "038bade4-7548-402d-93d8-2bd1a94aa06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MolformerModel(\n",
      "  (embeddings): MolformerEmbeddings(\n",
      "    (word_embeddings): Embedding(2362, 768, padding_idx=2)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (encoder): MolformerEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MolformerLayer(\n",
      "        (attention): MolformerAttention(\n",
      "          (self): MolformerSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (rotary_embeddings): MolformerRotaryEmbedding()\n",
      "            (feature_map): MolformerFeatureMap(\n",
      "              (kernel): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (output): MolformerSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MolformerIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MolformerOutput(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model from HuggingFace\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, deterministic_eval=True, trust_remote_code=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "905CGXEGiU8z"
   },
   "outputs": [],
   "source": [
    "# We need to add a regression head on the language model as we are doing a regression task.\n",
    "\n",
    "# specify model with a regression head\n",
    "\n",
    "class MoLFormerWithRegressionHead(nn.Module):\n",
    "    def __init__(self, model, hidden_size=768):\n",
    "        super(MoLFormerWithRegressionHead, self).__init__()\n",
    "        self.encoder = model\n",
    "\n",
    "        # regression head (fully connected layer)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.encoder(**inputs)\n",
    "        model_representation = outputs.pooler_output\n",
    "        regression_output = self.regressor(model_representation)\n",
    "        return regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiZjnOdTmOiQ",
    "outputId": "44825699-b5fe-4197-a3e3-156aa3876fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "MoLFormerWithRegressionHead(\n",
      "  (encoder): MolformerModel(\n",
      "    (embeddings): MolformerEmbeddings(\n",
      "      (word_embeddings): Embedding(2362, 768, padding_idx=2)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (encoder): MolformerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x MolformerLayer(\n",
      "          (attention): MolformerAttention(\n",
      "            (self): MolformerSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (rotary_embeddings): MolformerRotaryEmbedding()\n",
      "              (feature_map): MolformerFeatureMap(\n",
      "                (kernel): ReLU()\n",
      "              )\n",
      "            )\n",
      "            (output): MolformerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MolformerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MolformerOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (regressor): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=384, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the regression model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "print(device)\n",
    "print(regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o_s8JP3ilJr"
   },
   "source": [
    "## --- Step 4: Training ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5gzM6k9iWm2",
    "outputId": "f78cf2fd-8baf-43c5-cde7-a9319536e532"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:32<00:00,  6.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:25<00:00,  8.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0080\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code goes here\n",
    "def train(train_dataloader, model, tokenizer, epochs, loss_fn, optimizer, save_path):\n",
    "    model.train()\n",
    "    best_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        count = 0\n",
    "        with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "            for index, data in enumerate(pbar):\n",
    "                smile_strings = data[0]\n",
    "                smile_targets = data[1].to(device).float()\n",
    "\n",
    "                inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_fn(outputs, smile_targets)\n",
    "\n",
    "                running_loss = loss.item() + running_loss\n",
    "                count = count + len(data)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        epoch_loss = running_loss / count\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "        # saving the best model\n",
    "        if epoch_loss < best_loss :\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "learning_rate = 0.0001\n",
    "model_save_path = \"base_model.pth\"\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(\n",
    "    train_dataloader,\n",
    "    regression_model,\n",
    "    tokenizer,\n",
    "    epochs = epochs,\n",
    "    loss_fn=mse_loss,\n",
    "    optimizer=optimizer,\n",
    "    save_path=model_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSXUF4o0inpm"
   },
   "source": [
    "## --- Step 5: Evaluation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j55PSeCOAugw",
    "outputId": "7e1f0c48-66b5-406f-90f8-c22b856c2d1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-bfd94990e059>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  regression_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [00:02<00:00, 25.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5822\n",
      "Root Mean Squared Error (RMSE): 0.7630\n",
      "Mean Absolute Error (MAE): 0.5882\n",
      "R-squared (R¬≤): 0.5938\n",
      "Pearson Correlation: 0.8044\n",
      "Spearman Correlation: 0.7911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code goes here\n",
    "\n",
    "def test(test_dataloader, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "            for data in pbar:\n",
    "                smile_strings = data[0]\n",
    "                smile_targets = data[1].to(device).float()\n",
    "\n",
    "                inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "                outputs = model(inputs)  # Flatten output to match targets\n",
    "\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(smile_targets.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    predictions = np.array(predictions).flatten()  # Ensure 1D shape\n",
    "    actuals = np.array(actuals).flatten()\n",
    "\n",
    "    # Ensure correct dtype\n",
    "    predictions = predictions.astype(np.float64)\n",
    "    actuals = actuals.astype(np.float64)\n",
    "\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "    # Scale back\n",
    "    predictions = np.array(scaler.inverse_transform(predictions)).flatten().tolist()\n",
    "    actuals = np.array(scaler.inverse_transform(actuals)).flatten().tolist()\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    pearson_corr, _ = pearsonr(actuals, predictions)\n",
    "    spearman_corr, _ = spearmanr(actuals, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R-squared (R¬≤): {r2:.4f}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "regression_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "test(\n",
    "    test_dataloader,\n",
    "    regression_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RezJWenUDaKP"
   },
   "source": [
    "# 2.Add Unsupervised Finetuning\n",
    "In this step, you will perform unsupervised fine-tuning on the training dataset. This means the model will leverage only the SMILES strings without any corresponding labels to adapt its understanding of the data distribution. By familiarizing the model with the patterns and structure of the SMILES strings, you can potentially enhance its performance on downstream supervised tasks.\n",
    "\n",
    "For this fine-tuning, you will use the Masked Language Modeling (MLM) objective, where the model learns to predict randomly masked tokens within the input sequence. Remember to save the fine-tuned model for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282,
     "referenced_widgets": [
      "ea0f18af44524a48b707686da7d4a13f",
      "1e07f00e81ab4a9a8fe07a1d1edbb9f7",
      "5bfee8bba8bf4d978f203a672438a9df",
      "942ab69c435343c5b1660997dcdd41fd",
      "42cc122fa1874e0aa9c139f5aa4dfb3b",
      "1797893fda684685aff8b19b771362ca",
      "3cc391c2dce24e54970ad1c192abbb75",
      "a09e339f72654114ae13273ea2251225",
      "ae35b46604044746b498de3a42a874ae",
      "5a76ef90de41489bbe07b5a9f8a26d39",
      "51f02dffd703418286e6e5723afcfae6"
     ]
    },
    "id": "AdFowIrJuswR",
    "outputId": "9274d952-73d8-4348-feca-37197e490b6d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0f18af44524a48b707686da7d4a13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-18-3bc41bad01ed>:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Model saved to ./molformer_finetuned\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM, AutoTokenizer, Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# load the model for MLM with AutoModelForMaskedLM class\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", deterministic_eval=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/MoLFormer-XL-both-10pct\", trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# creating the traning data\n",
    "training_data = []\n",
    "for index, data in enumerate(train_dataloader):\n",
    "    smile_strings = list(data[0])\n",
    "    training_data.extend(smile_strings)\n",
    "\n",
    "dataset = Dataset.from_list([{\"smiles\": s} for s in training_data])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"smiles\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# creating the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# setting the traning arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./molformer_finetuned\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to = \"none\"\n",
    "    #no_cuda=True\n",
    ")\n",
    "\n",
    "# training the Masked LM on smile strings\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# saving the model\n",
    "model.save_pretrained(\"./molformer_finetuned\")\n",
    "tokenizer.save_pretrained(\"./molformer_finetuned\")\n",
    "\n",
    "print(\"Training completed! Model saved to ./molformer_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqkpS2Z828Dz",
    "outputId": "2c2017da-5e04-4677-850e-238bc7685f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Define the path where the model is saved\n",
    "model_path = \"./molformer_finetuned\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "\n",
    "print(\"Model and tokenizer successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chjQwDYEN83G"
   },
   "source": [
    "# 3.Fine-Tune for Comparison\n",
    "After performing unsupervised fine-tuning on the training data, we now fine-tune the model on the regression task with the regression head. By comparing the performance of the model before and after unsupervised fine-tuning, you can evaluate how the unsupervised fine-tuning impacts the model's performance on our target task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCgLdA2QabFY",
    "outputId": "aa7153b3-b30f-436d-e471-411534118845"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:29<00:00,  7.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:28<00:00,  7.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [00:27<00:00,  7.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0123\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code goes here\n",
    "def train(train_dataloader, model, tokenizer, epochs, loss_fn, optimizer, save_path):\n",
    "    model.train()\n",
    "    best_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        count = 0\n",
    "        with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "            for index, data in enumerate(pbar):\n",
    "                smile_strings = data[0]\n",
    "                smile_targets = data[1].to(device).float()\n",
    "\n",
    "                inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_fn(outputs, smile_targets)\n",
    "\n",
    "                running_loss = loss.item() + running_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "        if epoch_loss < best_loss :\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 0.0001\n",
    "model_save_path = \"unsupervised_model.pth\"\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(\n",
    "    train_dataloader,\n",
    "    regression_model,\n",
    "    tokenizer,\n",
    "    epochs = epochs,\n",
    "    loss_fn=mse_loss,\n",
    "    optimizer=optimizer,\n",
    "    save_path=model_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EomKWqRGabFY",
    "outputId": "5dd7a73f-bbc7-4c4f-e799-6ba678ea6422"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-03f9d2c7dcd6>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  regression_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [00:02<00:00, 23.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.4905\n",
      "Root Mean Squared Error (RMSE): 0.7004\n",
      "Mean Absolute Error (MAE): 0.5375\n",
      "R-squared (R¬≤): 0.6578\n",
      "Pearson Correlation: 0.8220\n",
      "Spearman Correlation: 0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code goes here\n",
    "def test(test_dataloader, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "            for data in pbar:\n",
    "                smile_strings = data[0]\n",
    "                smile_targets = data[1].to(device).float()\n",
    "\n",
    "                inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "                outputs = model(inputs)  # Flatten output to match targets\n",
    "\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(smile_targets.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    predictions = np.array(predictions).flatten()  # Ensure 1D shape\n",
    "    actuals = np.array(actuals).flatten()\n",
    "\n",
    "    # Ensure correct dtype\n",
    "    predictions = predictions.astype(np.float64)\n",
    "    actuals = actuals.astype(np.float64)\n",
    "\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "    # Scale back\n",
    "    predictions = np.array(scaler.inverse_transform(predictions)).flatten().tolist()\n",
    "    actuals = np.array(scaler.inverse_transform(actuals)).flatten().tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    pearson_corr, _ = pearsonr(actuals, predictions)\n",
    "    spearman_corr, _ = spearmanr(actuals, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R-squared (R¬≤): {r2:.4f}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "regression_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "test(\n",
    "    test_dataloader,\n",
    "    regression_model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "088c6e60eefac96a95ee1831e0427d780eec9d80e6c923c3459378d94bab5ddf"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1797893fda684685aff8b19b771362ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e07f00e81ab4a9a8fe07a1d1edbb9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1797893fda684685aff8b19b771362ca",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3cc391c2dce24e54970ad1c192abbb75",
      "value": "Map:‚Äá100%"
     }
    },
    "3cc391c2dce24e54970ad1c192abbb75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42cc122fa1874e0aa9c139f5aa4dfb3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51f02dffd703418286e6e5723afcfae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a76ef90de41489bbe07b5a9f8a26d39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bfee8bba8bf4d978f203a672438a9df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a09e339f72654114ae13273ea2251225",
      "max": 3360,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae35b46604044746b498de3a42a874ae",
      "value": 3360
     }
    },
    "942ab69c435343c5b1660997dcdd41fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a76ef90de41489bbe07b5a9f8a26d39",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_51f02dffd703418286e6e5723afcfae6",
      "value": "‚Äá3360/3360‚Äá[00:00&lt;00:00,‚Äá3836.84‚Äáexamples/s]"
     }
    },
    "a09e339f72654114ae13273ea2251225": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae35b46604044746b498de3a42a874ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ea0f18af44524a48b707686da7d4a13f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e07f00e81ab4a9a8fe07a1d1edbb9f7",
       "IPY_MODEL_5bfee8bba8bf4d978f203a672438a9df",
       "IPY_MODEL_942ab69c435343c5b1660997dcdd41fd"
      ],
      "layout": "IPY_MODEL_42cc122fa1874e0aa9c139f5aa4dfb3b"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9280964-50e0-42d0-b648-b78b7e91f687",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff0bc68-9306-4dd2-99ba-adc2d67f933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: accelerate==0.26.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.26.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (3.9.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.49.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from accelerate==0.26.0->-r requirements.txt (line 2)) (0.5.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 1)) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 7)) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2246da-256b-4ecc-832e-8b097e7e1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import datasets\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70640701-338a-4b8a-993e-a6835af46155",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"scikit-fingerprints/MoleculeNet_Lipophilicity\"\n",
    "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
    "\n",
    "ext_data = pd.read_csv(\"./tasks/External-Dataset_for_Task2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf1b835-9d80-453e-8db8-853ee01cf881",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Entry point\n",
    "########################################################7\n",
    "\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, strings, labels):\n",
    "        self.strings = strings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.strings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        string = self.strings[idx]\n",
    "        target = self.labels[idx]\n",
    "        return [string, target]\n",
    "\n",
    "class MoLFormerWithRegressionHead(nn.Module):\n",
    "    def __init__(self, model, hidden_size=768):\n",
    "        super(MoLFormerWithRegressionHead, self).__init__()\n",
    "        self.encoder = model\n",
    "\n",
    "        # regression head (fully connected layer)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.encoder(**inputs)\n",
    "        model_representation = outputs.pooler_output\n",
    "        regression_output = self.regressor(model_representation)\n",
    "        return regression_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aaeaf8-0c21-41f6-bd5d-1f1076aad5e5",
   "metadata": {},
   "source": [
    "## Influence score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7485c96-3ea4-4c4d-95b6-d6e90fddc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute gradients\n",
    "def compute_gradients(model, tokenizer, smile_strings, smile_targets, loss_fn):\n",
    "    \"\"\"Compute gradients of the loss with respect to model parameters.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = loss_fn(outputs, smile_targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad = torch.cat([param.grad.detach().view(-1) for param in model.parameters()])\n",
    "    return grad\n",
    "\n",
    "def lissa_approximation(model, tokenizer, train_loader, num_iters=100, damping=0.01):\n",
    "    model.eval()\n",
    "\n",
    "    cur_estimate = None  # Match first gradient shape\n",
    "    inv_hvp = None  # Store final inverse Hessian estimate\n",
    "\n",
    "    for iter_idx in tqdm(range(num_iters), desc=\"Estimating iHVP\"):\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            smile_strings, smile_targets = batch\n",
    "            smile_targets = smile_targets.to(device).float()\n",
    "\n",
    "            inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "            model.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.mse_loss(outputs, smile_targets)\n",
    "\n",
    "            grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "            grads = torch.cat([g.detach().view(-1) for g in grads])\n",
    "            \n",
    "\n",
    "            # Print gradient shapes for the first batch\n",
    "            if iter_idx == 0 and batch_idx == 0:\n",
    "                print(f\" Gradient Shape: {grads.shape}\")\n",
    "                cur_estimate = torch.zeros_like(grads)  # Match first gradient shape\n",
    "                inv_hvp = torch.zeros_like(grads) \n",
    "                print(f\" Initial cur_estimate shape: {cur_estimate.shape}\")\n",
    "\n",
    "            cur_estimate -= damping * grads\n",
    "        inv_hvp += cur_estimate  # Accumulate estimates\n",
    "\n",
    "    inv_hvp /= num_iters  # Normalize by iteration count to prevent oversized values\n",
    "    print(f\"** Final iHVP shape: {inv_hvp.shape}\")\n",
    "    return inv_hvp\n",
    "\n",
    "\n",
    "def compute_influence_scores(model, tokenizer, ext_loader, test_loader, loss_fn):\n",
    "    \n",
    "    # Compute inverse Hessian-vector product (iHVP)\n",
    "    iHVP = lissa_approximation(model, tokenizer, test_loader)\n",
    "\n",
    "    print(f\"Final iHVP Shape: {iHVP.shape}\")\n",
    "\n",
    "    # Compute influence scores\n",
    "    influence_scores = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(ext_loader):\n",
    "        smile_strings, smile_targets = batch\n",
    "        smile_targets = smile_targets.to(device).float()\n",
    "        ext_grad = compute_gradients(model, tokenizer, smile_strings, smile_targets, loss_fn)\n",
    "        # calculating the influence of single data point of the external data on test set\n",
    "        score = torch.dot(ext_grad.flatten(), iHVP)  \n",
    "        influence_scores.append((batch_idx, score.item()))\n",
    "        \n",
    "    return influence_scores\n",
    "\n",
    "def get_top_k(influence_scores, top_k):\n",
    "    # Convert to DataFrame\n",
    "    influence_df = pd.DataFrame(influence_scores, columns=[\"Index\", \"Influence Score\"])\n",
    "\n",
    "    # Add Influence Score column to ext_data and save it\n",
    "    ext_data[\"Influence Score\"] = None  # Initialize column\n",
    "    ext_data.loc[influence_df[\"Index\"], \"Influence Score\"] = influence_df[\"Influence Score\"].values\n",
    "    ext_data.to_csv(\"External_Data_with_influence_scores.csv\", index=False)\n",
    "    \n",
    "    # Keep only negative influence scores\n",
    "    influence_df = influence_df[influence_df[\"Influence Score\"] > 0]\n",
    "    print(f\"Number of positive samples: {len(influence_df)}\")\n",
    "\n",
    "    # Sort by influence score (descending)\n",
    "    influence_df = influence_df.sort_values(by=\"Influence Score\", ascending=False)\n",
    "\n",
    "    # Select the top influential samples\n",
    "    top_indices = influence_df.nlargest(top_k, \"Influence Score\")[\"Index\"]\n",
    "    selected_data = ext_data.loc[top_indices]\n",
    "\n",
    "    # Save selected data for fine-tuning\n",
    "    selected_data.to_csv(\"Selected_External_Data.csv\", index=False)\n",
    "    \n",
    "    top_k_indices = top_indices.tolist()\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747e4a4-7b58-48b0-a688-3cf77fa1cab3",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67abd2b-51ea-4cd8-9541-b489f28d43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(train_dataloader, model, tokenizer, epochs, loss_fn, optimizer, save_path):\n",
    "    model.train()\n",
    "    best_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        count = 0\n",
    "        with tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "            for index, data in enumerate(pbar):\n",
    "                smile_strings = data[0]\n",
    "                smile_targets = data[1].to(device).float()\n",
    "\n",
    "                inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_fn(outputs, smile_targets)\n",
    "\n",
    "                running_loss = loss.item() + running_loss\n",
    "                count = count + len(data)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        epoch_loss = running_loss / count\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # saving the model best model\n",
    "        if epoch_loss < best_loss :\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1802090c-a56b-45c8-b0ee-c6c8fb8cf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "def test(test_dataloader, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "            for data in pbar:\n",
    "                smile_strings = data[0]\n",
    "                smile_targets = data[1].to(device).float()\n",
    "\n",
    "                inputs = tokenizer(smile_strings, padding=True, return_tensors=\"pt\").to(device)\n",
    "                outputs = model(inputs)  # Flatten output to match targets\n",
    "\n",
    "                predictions.extend(outputs.cpu().numpy())\n",
    "                actuals.extend(smile_targets.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = np.array(actuals).flatten()\n",
    "    \n",
    "    # Ensure correct dtype\n",
    "    predictions = predictions.astype(np.float64)\n",
    "    actuals = actuals.astype(np.float64)\n",
    "\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    actuals = actuals.reshape(-1, 1)\n",
    "\n",
    "    # Scale back\n",
    "    predictions = np.array(scaler.inverse_transform(predictions)).flatten().tolist()\n",
    "    actuals = np.array(scaler.inverse_transform(actuals)).flatten().tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    pearson_corr, _ = pearsonr(actuals, predictions)\n",
    "    spearman_corr, _ = spearmanr(actuals, predictions)\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R-squared (R²): {r2:.4f}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77124778-af3a-46d1-962a-9dee11aefbe4",
   "metadata": {},
   "source": [
    "## Running the task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ff54bf-a205-433f-9269-83f182e5cbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# setting the seed\n",
    "\n",
    "seed = 100\n",
    "# Masked LM which is domain adapted with regression head from Task 1\n",
    "model_save_path = \"unsupervised_model.pth\"\n",
    "\n",
    "# for reproducibilty\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# downloading the dataset fro huggigface\n",
    "lipophilicity_df = pd.read_csv(\"hf://datasets/scikit-fingerprints/MoleculeNet_Lipophilicity/lipophilicity.csv\")\n",
    "\n",
    "# information regarding the dataset\n",
    "print(lipophilicity_df.shape)\n",
    "print(lipophilicity_df.info())\n",
    "print(lipophilicity_df.head())\n",
    "lipophilicity_strings = np.array(lipophilicity_df['SMILES'].values)\n",
    "lipophilicity_targets = np.array(lipophilicity_df['label'].values)\n",
    "\n",
    "# scaling the target values between (0,1)\n",
    "scaler = MinMaxScaler()\n",
    "lipophilicity_targets = scaler.fit_transform(lipophilicity_targets.reshape(-1,1))\n",
    "X_train, X_test, y_train_scaled, y_test_scaled = train_test_split(lipophilicity_strings, lipophilicity_targets, test_size = 0.2, random_state=seed)\n",
    "\n",
    "#ext_data = pd.read_csv(\"External-Dataset_for_Task2.csv\")\n",
    "\n",
    "# scaling the target values between (0,1) of external data\n",
    "ext_data_strings = ext_data['SMILES'].values\n",
    "ext_data_targets = np.array(ext_data['Label'].values)\n",
    "y_ext_scaled = scaler.fit_transform(ext_data_targets.reshape(-1,1))\n",
    "\n",
    "print(\"data reading complete\")\n",
    "\n",
    "# creating external dataloader\n",
    "ext_dataset = SMILESDataset(ext_data_strings, y_ext_scaled)\n",
    "ext_dataloader = DataLoader(ext_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# creating test dataloader\n",
    "test_dataset = SMILESDataset(X_test, y_test_scaled)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# loading the tokenizer and model \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, deterministic_eval=True, trust_remote_code=True)\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "regression_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# decalring loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "influences = compute_influence_scores(regression_model, tokenizer, ext_dataloader, test_dataloader, mse_loss)\n",
    "print(influences)\n",
    "top_k_indices = get_top_k(influences, top_k=20)\n",
    "top_k = 20\n",
    "print(f\"Top {top_k} positive influential training indices\")\n",
    "\n",
    "# taking a subset of the external data with top_k highest influence scores\n",
    "ext_dataset = Subset(ext_dataset, top_k_indices)\n",
    "train_dataset = SMILESDataset(X_train, y_train_scaled)\n",
    "combined_dataset = torch.utils.data.ConcatDataset([ext_dataset, train_dataset])\n",
    "combined_dataloader = DataLoader(combined_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# loading the model trained in Task 1 and creating new model with regression head\n",
    "model_path = \"./molformer_finetuned\"\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "\n",
    "epochs = 8\n",
    "learning_rate = 0.0001\n",
    "model_save_path = \"combined_model.pth\"\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(regression_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training\n",
    "train(\n",
    "    combined_dataloader,\n",
    "    regression_model,\n",
    "    tokenizer,\n",
    "    epochs = epochs,\n",
    "    loss_fn=mse_loss,\n",
    "    optimizer=optimizer,\n",
    "    save_path=model_save_path\n",
    ")\n",
    "\n",
    "# testing\n",
    "regression_model = MoLFormerWithRegressionHead(model).to(device)\n",
    "regression_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "test(\n",
    "    test_dataloader,\n",
    "    regression_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3ca2a-9209-4447-8803-469293bcb6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc5756-2648-4618-bfc4-40da01cb2896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Fine-Tuning Chemical Language Models with LoRA & Influence Sampling  
> *Parameter-Efficient Learning for Scientific Domains â€” NNTI Course Project, WS 2024/2025*  
> *By Hafiza Hajrah Rehman, Faria Alam, Muhammad Sabeeh Rehman*

[![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.13+-red?logo=pytorch)](https://pytorch.org)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Transformers-orange?logo=huggingface)](https://huggingface.co)
[![LoRA](https://img.shields.io/badge/LoRA-Parameter--Efficient-green)](https://arxiv.org/abs/2106.09685)

---

## ğŸ¯ Project Overview

Fine-tuned the **MolFormer** chemical language model (pretrained on PubChem/ZINC) for **lipophilicity prediction** using three advanced strategies:

1. **Masked Language Modeling (MLM)** â†’ Domain adaptation using SMILES strings  
2. **Influence Function-Based Data Selection** â†’ Curated 70 high-impact external samples to boost performance  
3. **Parameter-Efficient Fine-Tuning (PEFT)** â†’ Compared **LoRA, BitFit, iA3** vs full fine-tuning  

Demonstrated that **LoRA strikes the best balance** between performance and efficiency, while **influence scores require careful tuning** to avoid overestimation.

---

## ğŸ“Š Key Results

### ğŸ§¬ Influence Sampling (Task 2)
- Selected **top 70 positively influential samples** using influence functions (LiSSA approximation)  
- Achieved **synergistic performance boost** vs base model:  
  - **RMSE reduced by 12%**  
  - **RÂ² increased from 0.64 â†’ 0.68**  
- âš ï¸ Top 100 *negatively* scored samples outperformed top 50 *positively* scored â€” highlighting sensitivity of influence estimation

### ğŸ¯ Data Selection Strategies (Task 3a)
| Method              | RMSE   | RÂ²     | Spearman Ï |
|---------------------|--------|--------|-------------|
| **Diversity Sampling** | 0.6939 | 0.6801 | 0.8260      |
| Stratified Sampling | 0.7006 | 0.6726 | 0.8094      |
â†’ **Diversity sampling** slightly outperformed due to better generalization from varied SMILES patterns.

### âš™ï¸ Parameter-Efficient Tuning (Task 3b)
| Method       | Trainable Params | RÂ²   | RMSE  | Spearman Ï | Training Time |
|--------------|------------------|------|-------|-------------|---------------|
| Full FT      | 44.7M (100%)     | 0.72 | 0.65  | 0.85        | â³ Longest    |
| **LoRA**     | 885K (1.95%)     | 0.65 | 0.68  | 0.83        | â±ï¸ Moderate   |
| BitFit       | 370K (0.82%)     | 0.55 | 0.75  | 0.78        | â±ï¸ Fast       |
| iA3          | 295K (0.66%)     | 0.50 | 0.80  | 0.75        | â±ï¸ Fastest    |

âœ… **LoRA is the practical winner** â€” near-full-FT performance with 98% fewer parameters.  
âš ï¸ **iA3 is fastest but least accurate** â€” suitable only when speed >> accuracy.

---

## ğŸ§° Project Structure & Setup

```
chem-llm-lora/
â”œâ”€â”€ Task1.ipynb           # Masked LM + Base Fine-tuning
â”œâ”€â”€ Task2.ipynb           # Influence Function Scoring + Data Selection
â”œâ”€â”€ Task3.ipynb
â”œâ”€â”€ requirements.txt      # Dependencies for Task 3
â”‚â”€â”€ data/                 # Selected external samples (from Task 2)
â”œâ”€â”€ models/               # Saved models (Task 1 & 2)
â”œâ”€â”€ External_data_with_influence_score           
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

### âœ… Task 1: Masked LM & Base Fine-tuning
```bash
# Run in Colab or Jupyter
jupyter notebook Task1.ipynb
```
> Installs dependencies, trains MolFormer with MLM, saves models to `./models/`

---

### âœ… Task 2: Influence Score Calculation
```bash
# MUST run Task1.ipynb first!
jupyter notebook Task2.ipynb
```
> Computes influence scores for 300 external samples, saves top samples and scores to `./results/` and `./data/`

---

### âœ… Task 3: Parameter-Efficient Fine-Tuning
```bash
# Create new environment (recommended)
python -m venv nnti-env
source nnti-env/bin/activate  # Linux/Mac
# nnti-env\Scripts\activate   # Windows

# Install dependencies
pip install -r Task3/requirements.txt

# Run training (e.g., LoRA)
cd Task3
python train.py --method lora --data_path ../data/selected_samples.csv

# Other methods: --method bitfit, --method ia3, --method full
```

> âœ… Ensure `Task3/data/` contains selected samples from Task 2.

---

## ğŸ“š Dataset

- **Primary**: MoleculeNet Lipophilicity (4,200 SMILES strings + normalized logP values)  
- **External**: 300 additional SMILES samples for influence-based selection  
- **Source**: [Hugging Face - MoleculeNet Lipophilicity](https://huggingface.co/datasets/scikit-fingerprints/MoleculeNet_Lipophilicity)

---

## ğŸ‘©â€ğŸ’» About Me

**Hafiza Hajrah Rehman**  
M.Sc. Data Science & AI @ Saarland University  
Specializing in Trustworthy AI, Adversarial ML, Model Interpretability  
ğŸ™ [GitHub](https://github.com/hajraRehman) | âœ‰ï¸ hafizahajra6@gmail.com

---

ğŸ“„ **View Full Report PDF**: [NNTI_2025_project_report.pdf](NNTI_2025_project_report.pdf)

_Last updated: Sept 2025_
